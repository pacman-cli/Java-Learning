spring.application.name=customer-support
# Server
#server.port=8080
#My SQL
spring.datasource.url=${SPRING_DATASOURCE_URL}
spring.datasource.username=${SPRING_DATASOURCE_USERNAME}
spring.datasource.password=${SPRING_DATASOURCE_PASSWORD}
spring.jpa.hibernate.ddl-auto=${SPRING_JPA_HIBERNATE_DDL_AUTO:update}
server.port=${PORT:8080}

# MySQL
#spring.datasource.url=jdbc:mysql://localhost:3306/customer_support_ai?useSSL=false&serverTimezone=UTC&allowPublicKeyRetrieval=true
#spring.datasource.username=root
#spring.datasource.password=MdAshikur123+
#spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver

# JPA
#spring.jpa.hibernate.ddl-auto=update
spring.jpa.show-sql=true
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQLDialect
spring.jpa.properties.hibernate.format_sql=true

# Ollama Configuration (Free Local LLM)
# ===============================
# IMPORTANT: If you get "model not found" errors, make sure you have pulled the models:
# For chat: ollama pull llama3.2:1b
# For embeddings (optional): ollama pull nomic-embed-text
# 
# To see available models: ollama list
# To pull a new model: ollama pull <model-name>
# 
# Base URL for Ollama API server - default port is 11434
# This connects to your local Ollama instance running on your machine
spring.ai.ollama.base-url=http://localhost:11434

# Chat Model Configuration
# Specifies which model to use for chat/text generation
# Must match exactly with a model you've pulled using "ollama pull <model-name>"
spring.ai.ollama.chat.options.model=llama3.2:1b

# Temperature controls randomness in responses (0.0 = deterministic, 1.0 = creative)
# Lower values = more focused/consistent, Higher values = more creative/varied
spring.ai.ollama.chat.options.temperature=0.7

# Embedding Model Configuration
# Using dedicated embedding model for better vector similarity search
# nomic-embed-text is optimized specifically for embeddings (better than using chat models)
spring.ai.ollama.embedding.options.model=nomic-embed-text

# Additional Ollama Options (uncomment to use)
# ===============================
# Maximum tokens in response (default: no limit)
# spring.ai.ollama.chat.options.max-tokens=2048

# Top-p sampling (nucleus sampling) - controls diversity (0.1 = focused, 0.9 = diverse)
# spring.ai.ollama.chat.options.top-p=0.9

# Frequency penalty - reduces repetition (-2.0 to 2.0)
# spring.ai.ollama.chat.options.frequency-penalty=0.0

# Presence penalty - encourages new topics (-2.0 to 2.0)
# spring.ai.ollama.chat.options.presence-penalty=0.0

# Request timeout in seconds
# spring.ai.ollama.chat.options.timeout=60s

# Logging Configuration
# ===============================
# Spring AI logging for debugging
logging.level.org.springframework.ai=INFO
# Application-specific logging
logging.level.com.pacman.customersupport=DEBUG
# Ollama API logging (set to DEBUG for detailed API calls)
logging.level.org.springframework.ai.ollama=INFO
# SQL logging (already enabled above in JPA section)
# logging.level.org.hibernate.SQL=DEBUG

# = SWAGGER (SpringDoc)
# ===============================
springdoc.api-docs.path=/api-docs
springdoc.swagger-ui.path=/swagger-ui.html